{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6331922",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "407694b0",
   "metadata": {},
   "source": [
    "# üí° LLM + RAG Interview Guide\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ LLM Fundamentals\n",
    "\n",
    "### ‚û§ What is tokenization, and how does it affect generation?\n",
    "Tokenization is the process of breaking input text into smaller units (tokens), such as words, subwords, or characters. LLMs process and generate outputs token-by-token, so the type of tokenizer (like BPE or WordPiece) impacts:\n",
    "- Model input length (affecting cost and speed)\n",
    "- Output fluency and formatting\n",
    "- Memory and computation needs\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do embeddings really work?\n",
    "Embeddings are vector representations of tokens or text chunks. They capture **semantic relationships**:\n",
    "- Similar meanings ‚Üí closer vectors\n",
    "- LLMs convert token IDs into embeddings as input\n",
    "- These vectors are passed through transformer layers to understand context\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ What‚Äôs the role of attention and positional encoding?\n",
    "- **Attention**: Helps the model decide \"what to focus on\" at each token. It gives weight to more relevant tokens in context.\n",
    "- **Positional Encoding**: Since transformers process input in parallel, they need position information (e.g., order of words). Positional encodings inject this order awareness.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ What changes during fine-tuning? (optimizers, schedulers, layer freezing)\n",
    "- **Optimizers** like AdamW are used to update weights.\n",
    "- **Schedulers** control learning rate decay.\n",
    "- **Layer freezing** can keep earlier layers fixed while updating only the later ones to avoid catastrophic forgetting and save compute.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ LoRA vs QLoRA vs Full Fine-tune ‚Äì Tradeoffs\n",
    "\n",
    "| Method       | Pros                           | Cons                           |\n",
    "|--------------|--------------------------------|--------------------------------|\n",
    "| Full Fine-tune | Best performance             | Very compute and memory heavy |\n",
    "| LoRA         | Lightweight, efficient         | Needs adapter injection        |\n",
    "| QLoRA        | Most memory-efficient (4-bit)  | Slight tradeoff in accuracy    |\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Prompting & Context Engineering\n",
    "\n",
    "### ‚û§ Few-shot vs Zero-shot ‚Äì Which works better where?\n",
    "- **Zero-shot**: Best for general-purpose tasks with clear instructions (e.g., classification).\n",
    "- **Few-shot**: Better for nuanced, domain-specific, or creative tasks where examples help steer behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you design system prompts that are robust across users?\n",
    "- Use clear, concise instructions.\n",
    "- Define expected format and tone.\n",
    "- Include edge-case handling.\n",
    "- Use consistent structure to reduce ambiguity.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you make output deterministic?\n",
    "- Set **temperature = 0** and **top_p = 1.0**\n",
    "- Use consistent prompts\n",
    "- Fix random seeds (for some APIs)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you track, version, and backfill changing context?\n",
    "- Use versioned templates or prompt IDs\n",
    "- Store prompt history with timestamps\n",
    "- Backfill by rerunning previous inputs with updated contexts\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you build/maintain memory?\n",
    "- Use vector databases or key-value stores\n",
    "- Index past interactions by session/user\n",
    "- Retrieve relevant past interactions per new query\n",
    "- Summarize or prune long-term memory\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ RAG Systems\n",
    "\n",
    "### ‚û§ What‚Äôs your chunking strategy ‚Äì by length, semantics, or structure?\n",
    "- **Structure-first** (e.g., paragraphs or sections)\n",
    "- **Length-bounded** (e.g., 500-800 tokens)\n",
    "- **Semantic overlap** ensures smoother context continuity\n",
    "\n",
    "Use tools like `RecursiveCharacterTextSplitter` in LangChain for hybrid strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you choose a vector DB?\n",
    "| DB       | When to Use                             |\n",
    "|----------|------------------------------------------|\n",
    "| Chroma   | Local dev, light RAG prototypes          |\n",
    "| Pinecone | Production-scale vector retrieval        |\n",
    "| OpenSearch | Combine keyword + vector search       |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ Can you update or backfill embeddings with zero downtime?\n",
    "Yes:\n",
    "- Shadow indexing\n",
    "- Background jobs for re-embedding\n",
    "- Dual index systems (old vs new) with hot swap\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you evaluate retrieval quality?\n",
    "- **Precision@k, Recall@k, MRR**\n",
    "- **Manual eval** for semantic correctness\n",
    "- **Reranking models** (cross-encoders)\n",
    "- **Citations** to trace source chunks\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ MLOps & LLMOps\n",
    "\n",
    "### ‚û§ Sketch a pipeline: raw data ‚Üí model ‚Üí serving ‚Üí feedback\n",
    "1. Ingest raw/unstructured data\n",
    "2. Preprocess & embed\n",
    "3. Fine-tune or prompt-template\n",
    "4. Deploy (API or batch)\n",
    "5. Log outputs & collect feedback\n",
    "6. Monitor & retrain\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How would you monitor performance drift or hallucinations?\n",
    "- Track similarity between response and ground truth\n",
    "- Run named entity/fact checkers\n",
    "- Score outputs for domain deviation\n",
    "- Use feedback ratings\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ How do you log prompts and outputs?\n",
    "- Structured logging (JSON)\n",
    "- Include: prompt ID, input, output, model, temperature\n",
    "- Store in databases or logging platforms (e.g., ELK, LangSmith)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ CI/CD for LLM workflows ‚Äì What‚Äôs different?\n",
    "- Includes prompt testing and versioning\n",
    "- Need evaluation of output quality, bias, hallucination\n",
    "- Human-in-the-loop validation\n",
    "- Rollbacks for prompt templates or chains\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Cost & Latency Tradeoffs\n",
    "\n",
    "### ‚û§ How do you reduce token usage?\n",
    "- Minimize prompt length\n",
    "- Compress context with embeddings\n",
    "- Avoid over-engineering system prompts\n",
    "- Use summarization for history\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ When should you quantize a model?\n",
    "- On edge devices (mobile, embedded systems)\n",
    "- When cost, latency, or memory constraints apply\n",
    "- For faster inference with minimal accuracy loss\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ Batching & caching strategy?\n",
    "- Batch requests to GPUs (e.g., using Hugging Face or Triton)\n",
    "- Cache embeddings, frequent prompts/responses\n",
    "- Use async APIs to parallelize latency\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ Hosted APIs vs open-source models?\n",
    "- **Hosted APIs**: Easier, scalable, costlier in long term\n",
    "- **Open-source models**: More control, cost-effective, but higher DevOps effort\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ System Design Thinking\n",
    "\n",
    "### ‚û§ How to make AI systems more deterministic?\n",
    "- Fix temperature, top_p\n",
    "- Use prompt chains with fallback logic\n",
    "- Unit test with edge case prompts\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ What fallback do you use if LLM fails?\n",
    "- Rule-based system\n",
    "- Predefined templates\n",
    "- Human-in-the-loop escalation\n",
    "- Cache valid responses\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ Can you solve this without LLM or vector DB?\n",
    "Yes:\n",
    "- Use regex or rule-based NLU for structured tasks\n",
    "- SQL or keyword search for fixed-structure data\n",
    "\n",
    "---\n",
    "\n",
    "### ‚û§ Right database: SQL, NoSQL, or vector?\n",
    "| Type      | Use Case                            |\n",
    "|-----------|-------------------------------------|\n",
    "| SQL       | Structured, relational data         |\n",
    "| NoSQL     | Flexible, document-based storage    |\n",
    "| Vector DB | Semantic similarity, embeddings, RAG|\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Real-World Scenarios\n",
    "\n",
    "### 1Ô∏è‚É£ What happens if your embedding model changes?\n",
    "- Recompute embeddings for all docs\n",
    "- Use background jobs for re-indexing\n",
    "- Run A/B tests with shadow index\n",
    "- Cutover when stable\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ How would you fine-tune a model on user behavior?\n",
    "- Log interactions\n",
    "- Label data from feedback\n",
    "- Train with supervised fine-tuning\n",
    "- Validate on holdout set\n",
    "- Deploy with version control\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How to make the system cheaper?\n",
    "- Use distilled/smaller models for simple tasks\n",
    "- Cache frequent queries\n",
    "- Use RAG instead of stuffing long context\n",
    "- Quantize model (int8 or QLoRA)\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Debugging LLM outputs?\n",
    "- Review prompt + output logs\n",
    "- Check model version + config (temp, top_p)\n",
    "- Reproduce in isolation\n",
    "- Check for hallucinations, prompt injection\n",
    "- Iterate on prompt design or retrieval context\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899f6db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577982be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
